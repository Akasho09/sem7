{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 7: Hierarchical Agglomerative Text Clustering (HAC / AGNES)\n", "\n", "This notebook implements **Hierarchical Agglomerative Clustering (HACL)** for a Twitter text dataset, following the lab instructions:\n", "\n", "1. Load the tweets\n", "2. Preprocess text (tokenization, lemmatization, stopword removal, etc.)\n", "3. Compute the term\u2013document matrix (TF\u2011IDF)\n", "4. Compute distance matrix\n", "5. Perform Hierarchical Agglomerative Clustering\n", "6. Draw the dendrogram\n", "7. Interpret clustering\n", "8. Compare clusters with existing class labels\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix\n", "from sklearn.preprocessing import LabelEncoder\n", "\n", "from scipy.cluster.hierarchy import dendrogram, linkage\n", "from scipy.spatial.distance import pdist\n", "\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.stem import WordNetLemmatizer\n", "import re\n", "\n", "nltk.download('punkt')\n", "nltk.download('stopwords')\n", "nltk.download('wordnet')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Load the Tweets Dataset\n", "\n", "\ud83d\udc49 **Note:** Replace `tweets_3class.csv` with your actual file name / path.\n", "The dataset is expected to have at least:\n", "- a text column (e.g. `text` or `tweet`)\n", "- a label column (e.g. `label` or `class`)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# TODO: change file name if needed\n", "df = pd.read_csv('tweets_3class.csv')  # <-- replace with your dataset filename\n", "\n", "print(df.head())\n", "print(\"\\nColumns:\", df.columns.tolist())\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set the names of the **text column** and **label column** here so that the rest of the notebook works with your dataset."]}, {"cell_type": "code", "metadata": {}, "source": ["# Set these to match your dataset\n", "TEXT_COL = 'text'   # e.g. 'text', 'tweet'\n", "LABEL_COL = 'label' # e.g. 'label', 'class'\n", "\n", "texts = df[TEXT_COL].astype(str).values\n", "labels_raw = df[LABEL_COL].astype(str).values\n", "\n", "print(\"Number of samples:\", len(texts))\n", "print(\"Unique labels:\", np.unique(labels_raw))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. NLP Pre\u2011processing\n", "\n", "- Lower\u2011casing\n", "- Removing URLs, mentions, hashtags, digits, punctuation\n", "- Tokenization\n", "- Stopword removal\n", "- Lemmatization\n"]}, {"cell_type": "code", "metadata": {}, "source": ["stop_words = set(stopwords.words('english'))\n", "lemmatizer = WordNetLemmatizer()\n", "\n", "def clean_tweet(text: str) -> str:\n", "    # lower case\n", "    text = text.lower()\n", "    # remove urls\n", "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n", "    # remove mentions and hashtags\n", "    text = re.sub(r'[@#]\\w+', ' ', text)\n", "    # keep only letters\n", "    text = re.sub(r'[^a-z\\s]', ' ', text)\n", "    # tokenize\n", "    tokens = nltk.word_tokenize(text)\n", "    # remove stopwords and short tokens, then lemmatize\n", "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens\n", "              if tok not in stop_words and len(tok) > 2]\n", "    return ' '.join(tokens)\n", "\n", "clean_texts = [clean_tweet(t) for t in texts]\n", "df['clean_text'] = clean_texts\n", "df[['clean_text', LABEL_COL]].head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Term\u2011Document Matrix (TF\u2011IDF)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["vectorizer = TfidfVectorizer(max_features=5000)\n", "X_tfidf = vectorizer.fit_transform(clean_texts)\n", "X_tfidf.shape\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Distance Matrix (Cosine Distance)\n", "\n", "We compute pairwise cosine distances on the TF\u2011IDF vectors."]}, {"cell_type": "code", "metadata": {}, "source": ["# condensed distance matrix required for linkage/dendrogram\n", "distance_matrix = pdist(X_tfidf.toarray(), metric='cosine')\n", "distance_matrix.shape\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Perform Hierarchical Agglomerative Clustering\n", "\n", "We start with each tweet as its own cluster, and iteratively merge closest clusters\nusing a chosen **linkage** criterion (e.g. `average`, `complete`, `single`, `ward`).\n"]}, {"cell_type": "code", "metadata": {}, "source": ["linkage_method = 'average'  # try 'single', 'complete', 'ward' too (ward needs euclidean)\n", "Z = linkage(distance_matrix, method=linkage_method)\n", "Z[:5]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Draw the Dendrogram\n"]}, {"cell_type": "code", "metadata": {}, "source": ["plt.figure(figsize=(12, 5))\n", "dendrogram(Z, truncate_mode='level', p=5)\n", "plt.title(f'Dendrogram (linkage={linkage_method})')\n", "plt.xlabel('Sample index or (cluster size)')\n", "plt.ylabel('Distance')\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Cut the Dendrogram into k Clusters and Interpret\n", "\n", "We now choose a number of clusters `k` (e.g. 3 classes as mentioned in the lab) and\nobtain cluster labels.  We then compare them to the true labels in the dataset.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from scipy.cluster.hierarchy import fcluster\n", "\n", "k = 3  # number of target clusters (change if needed)\n", "cluster_labels = fcluster(Z, k, criterion='maxclust')\n", "\n", "df['cluster'] = cluster_labels\n", "df[[TEXT_COL, 'clean_text', LABEL_COL, 'cluster']].head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Compare Clusters with Existing Class Labels\n", "\n", "We use:\n", "- Confusion matrix\n", "- Adjusted Rand Index (ARI)\n", "- Silhouette score (internal clustering quality)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# Encode true labels as integers for comparison\n", "le = LabelEncoder()\n", "y_true = le.fit_transform(labels_raw)\n", "y_cluster = cluster_labels\n", "\n", "cm = confusion_matrix(y_true, y_cluster)\n", "ari = adjusted_rand_score(y_true, y_cluster)\n", "sil = silhouette_score(X_tfidf, y_cluster, metric='cosine')\n", "\n", "print('Confusion Matrix (rows=true, cols=cluster):')\n", "print(cm)\n", "print('\\nAdjusted Rand Index (higher better, 0=random):', ari)\n", "print('Silhouette score (higher better):', sil)\n", "\n", "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n", "plt.xlabel('Cluster label')\n", "plt.ylabel('True label')\n", "plt.title('Confusion Matrix: True vs Cluster Labels')\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Simple Cluster Interpretation\n", "\n", "To interpret each cluster, we can look at a few example tweets from each cluster\nand also show the top TF\u2011IDF terms for that cluster.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["feature_names = np.array(vectorizer.get_feature_names_out())\n", "\n", "for c in range(1, k+1):\n", "    print(f\"\\n=== Cluster {c} ===\")\n", "    idx = np.where(cluster_labels == c)[0]\n", "    print(f\"Number of tweets: {len(idx)}\")\n", "    \n", "    # Show a few example tweets\n", "    for t in df.iloc[idx][:3][TEXT_COL]:\n", "        print(\"-\", t)\n", "    \n", "    # Compute mean TF-IDF for this cluster and get top terms\n", "    mean_tfidf = X_tfidf[idx].mean(axis=0).A1\n", "    top_idx = mean_tfidf.argsort()[::-1][:10]\n", "    print(\"Top terms:\", feature_names[top_idx])\n"]}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 2}